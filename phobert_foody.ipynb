{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms, utils, models\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time\nfrom torch.autograd import Variable\nimport pandas as pd\nimport os\nos.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\nimport pandas as pd\nimport torch\nimport numpy\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.svm import SVC\nfrom joblib import dump\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.utils.data.dataloader import default_collate\nfrom transformers import XLMRobertaTokenizer, XLMRobertaXLModel\nfrom transformers import XLNetTokenizer, XLNetModel\nimport random\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision.ops.focal_loss as fl\nimport tensorflow_addons as tfa\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-15T07:14:01.295869Z","iopub.execute_input":"2022-12-15T07:14:01.296385Z","iopub.status.idle":"2022-12-15T07:14:11.373932Z","shell.execute_reply.started":"2022-12-15T07:14:01.296278Z","shell.execute_reply":"2022-12-15T07:14:11.372868Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n.datatable .frame thead tr.colnames {  background-image: url('data:image/png;base64,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');  background-repeat: repeat-x;  background-size: 14px;  height: 28px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"TRAIN1_DIR = '/kaggle/input/foody-data-after-pre/pre_train.csv'\nTRAIN2_DIR = '/kaggle/input/foody-data-after-pre/data2_processed.csv'\nTEST_DIR = '/kaggle/input/foody-data-after-pre/pre_test.csv'\nVAL_DIR = '/kaggle/input/foody-data-after-pre/val_labelled.csv'","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.376111Z","iopub.execute_input":"2022-12-15T07:14:11.376506Z","iopub.status.idle":"2022-12-15T07:14:11.384738Z","shell.execute_reply.started":"2022-12-15T07:14:11.376467Z","shell.execute_reply":"2022-12-15T07:14:11.380315Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(25)\nnp.random.seed(25)\nrandom.seed(25)\n\ncudnn.benchmark = True\ncudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.386236Z","iopub.execute_input":"2022-12-15T07:14:11.386687Z","iopub.status.idle":"2022-12-15T07:14:11.405912Z","shell.execute_reply.started":"2022-12-15T07:14:11.386642Z","shell.execute_reply":"2022-12-15T07:14:11.405043Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(TRAIN1_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.409177Z","iopub.execute_input":"2022-12-15T07:14:11.409907Z","iopub.status.idle":"2022-12-15T07:14:11.569089Z","shell.execute_reply.started":"2022-12-15T07:14:11.409865Z","shell.execute_reply":"2022-12-15T07:14:11.568103Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = df.drop(\"Unnamed: 0\", axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.570452Z","iopub.execute_input":"2022-12-15T07:14:11.570823Z","iopub.status.idle":"2022-12-15T07:14:11.587659Z","shell.execute_reply.started":"2022-12-15T07:14:11.570787Z","shell.execute_reply":"2022-12-15T07:14:11.586771Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df['Rating'] = df['Rating'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.589639Z","iopub.execute_input":"2022-12-15T07:14:11.590075Z","iopub.status.idle":"2022-12-15T07:14:11.598189Z","shell.execute_reply.started":"2022-12-15T07:14:11.590040Z","shell.execute_reply":"2022-12-15T07:14:11.597270Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.599692Z","iopub.execute_input":"2022-12-15T07:14:11.600200Z","iopub.status.idle":"2022-12-15T07:14:11.664620Z","shell.execute_reply.started":"2022-12-15T07:14:11.600165Z","shell.execute_reply":"2022-12-15T07:14:11.663476Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df['Comment'] = df['Comment'].str.strip()","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.666340Z","iopub.execute_input":"2022-12-15T07:14:11.667056Z","iopub.status.idle":"2022-12-15T07:14:11.687097Z","shell.execute_reply.started":"2022-12-15T07:14:11.667018Z","shell.execute_reply":"2022-12-15T07:14:11.686138Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df = df[df['Comment']  != 'None']","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.688749Z","iopub.execute_input":"2022-12-15T07:14:11.689346Z","iopub.status.idle":"2022-12-15T07:14:11.696751Z","shell.execute_reply.started":"2022-12-15T07:14:11.689305Z","shell.execute_reply":"2022-12-15T07:14:11.695810Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# df_train, df_test = train_test_split(df, test_size=0.15, random_state=42)\n\ndf_train = df\ndf_val = pd.read_csv(VAL_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.701359Z","iopub.execute_input":"2022-12-15T07:14:11.701672Z","iopub.status.idle":"2022-12-15T07:14:11.788250Z","shell.execute_reply.started":"2022-12-15T07:14:11.701634Z","shell.execute_reply":"2022-12-15T07:14:11.787282Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_val['Rating'] = df_val['Rating'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.789794Z","iopub.execute_input":"2022-12-15T07:14:11.790411Z","iopub.status.idle":"2022-12-15T07:14:11.795880Z","shell.execute_reply.started":"2022-12-15T07:14:11.790373Z","shell.execute_reply":"2022-12-15T07:14:11.794716Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_2 = pd.read_csv(TRAIN2_DIR)\ndf_2 = df_2.drop(\"Unnamed: 0\", axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.797395Z","iopub.execute_input":"2022-12-15T07:14:11.798196Z","iopub.status.idle":"2022-12-15T07:14:11.908420Z","shell.execute_reply.started":"2022-12-15T07:14:11.798156Z","shell.execute_reply":"2022-12-15T07:14:11.907482Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df_2['Rating'] = df_2['Rating'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.909952Z","iopub.execute_input":"2022-12-15T07:14:11.910323Z","iopub.status.idle":"2022-12-15T07:14:11.916140Z","shell.execute_reply.started":"2022-12-15T07:14:11.910287Z","shell.execute_reply":"2022-12-15T07:14:11.914971Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_0_2 = df_2[df_2['Rating'] == 0].sample(1450)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.917918Z","iopub.execute_input":"2022-12-15T07:14:11.918509Z","iopub.status.idle":"2022-12-15T07:14:11.927089Z","shell.execute_reply.started":"2022-12-15T07:14:11.918474Z","shell.execute_reply":"2022-12-15T07:14:11.926255Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_0_2['Rating'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.928830Z","iopub.execute_input":"2022-12-15T07:14:11.929179Z","iopub.status.idle":"2022-12-15T07:14:11.941485Z","shell.execute_reply.started":"2022-12-15T07:14:11.929145Z","shell.execute_reply":"2022-12-15T07:14:11.940575Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0    1450\nName: Rating, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_1_2 = df_2[df_2['Rating'] == 1].sample(1000)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.943027Z","iopub.execute_input":"2022-12-15T07:14:11.943284Z","iopub.status.idle":"2022-12-15T07:14:11.949871Z","shell.execute_reply.started":"2022-12-15T07:14:11.943261Z","shell.execute_reply":"2022-12-15T07:14:11.948877Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_train = pd.concat([df_train[['Comment','Rating']],\n                df_0_2[['Comment','Rating']],\n                df_1_2[['Comment','Rating']]\n               ])","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.951361Z","iopub.execute_input":"2022-12-15T07:14:11.952587Z","iopub.status.idle":"2022-12-15T07:14:11.962364Z","shell.execute_reply.started":"2022-12-15T07:14:11.952551Z","shell.execute_reply":"2022-12-15T07:14:11.961443Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.964412Z","iopub.execute_input":"2022-12-15T07:14:11.965135Z","iopub.status.idle":"2022-12-15T07:14:11.971649Z","shell.execute_reply.started":"2022-12-15T07:14:11.965100Z","shell.execute_reply":"2022-12-15T07:14:11.970577Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(11519, 2)"},"metadata":{}}]},{"cell_type":"code","source":"df_val['Rating'] = df_val['Rating'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.973047Z","iopub.execute_input":"2022-12-15T07:14:11.973485Z","iopub.status.idle":"2022-12-15T07:14:11.980954Z","shell.execute_reply.started":"2022-12-15T07:14:11.973451Z","shell.execute_reply":"2022-12-15T07:14:11.980040Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import shuffle","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.982542Z","iopub.execute_input":"2022-12-15T07:14:11.982983Z","iopub.status.idle":"2022-12-15T07:14:11.989688Z","shell.execute_reply.started":"2022-12-15T07:14:11.982950Z","shell.execute_reply":"2022-12-15T07:14:11.988791Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop_duplicates(['Comment'])","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:11.991187Z","iopub.execute_input":"2022-12-15T07:14:11.991523Z","iopub.status.idle":"2022-12-15T07:14:12.034450Z","shell.execute_reply.started":"2022-12-15T07:14:11.991490Z","shell.execute_reply":"2022-12-15T07:14:12.033575Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df_train['Rating'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:12.036397Z","iopub.execute_input":"2022-12-15T07:14:12.037251Z","iopub.status.idle":"2022-12-15T07:14:12.046981Z","shell.execute_reply.started":"2022-12-15T07:14:12.037215Z","shell.execute_reply":"2022-12-15T07:14:12.045906Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"1    8122\n0    3348\nName: Rating, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"### model","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\n# wonrax/phobert-base-vietnamese-sentiment\nclass BERT_classification(nn.Module):\n    def __init__(self):\n        super(BERT_classification, self).__init__()\n        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\").to(device)\n        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n        self.classifier = nn.Sequential(\n          nn.Dropout(p=0.2),\n          nn.Linear(in_features=768, out_features = 256, bias=True).to(device),\n          nn.Dropout(p=0.1),\n          nn.Linear(in_features=256, out_features = 2, bias=True).to(device)\n        )\n        self.criterior = nn.CrossEntropyLoss()\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        param_optimizer = list(self.bert.named_parameters())\n        self.optimizer =optim.AdamW(\n            [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n            {'params': self.classifier.parameters(), 'lr': 8e-5}],\n            lr = 2e-5\n        )\n\n    def tokenize(self,v_text, label):\n        v_tokenized = []\n        max_len = 256\n        for i_text in v_text:\n            i_text = standardize_data(i_text)\n            i_text = self.choose_sub_text(i_text)\n            line = self.tokenizer.encode(i_text, max_length = max_len, truncation=True)\n            v_tokenized.append(line)\n\n        padded = numpy.array([i + [1] * (max_len - len(i)) for i in v_tokenized])\n        attention_mask = numpy.where(padded == 1, 0, 1)\n        padded = torch.tensor(padded).to(torch.long)\n        attention_mask = torch.tensor(attention_mask)\n        \n        target = torch.tensor(label) \n        data_tensor = TensorDataset(padded, target) \n        train_loader = DataLoader(dataset = data_tensor, batch_size = 32)\n        attention_mask = DataLoader(attention_mask, batch_size = 32)\n        \n        return train_loader, attention_mask\n    \n    def choose_sub_text(self, text):\n        tmp = len(text.split())\n        if tmp > 256:\n            words = text.split()\n            text = ' '.join(words[0:210]) + ' ' + ' '.join(words[-46:])\n            \n        return text\n    \n    def forward(self, input_ids, attention_mask):\n        _, output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n             return_dict=False\n        )\n        output = self.classifier(output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:12.048718Z","iopub.execute_input":"2022-12-15T07:14:12.049318Z","iopub.status.idle":"2022-12-15T07:14:12.064568Z","shell.execute_reply.started":"2022-12-15T07:14:12.049283Z","shell.execute_reply":"2022-12-15T07:14:12.063552Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def standardize_data(row):\n    row = re.sub(r\"[\\.,\\?]+$-\", \"\", row)\n    row = row.replace(\",\", \" \").replace(\".\", \" \") \\\n        .replace(\";\", \" \").replace(\"“\", \" \") \\\n        .replace(\":\", \" \").replace(\"”\", \" \") \\\n        .replace('\"', \" \").replace(\"'\", \" \") \\\n        .replace(\"!\", \" \").replace(\"?\", \" \") \\\n        .replace(\"-\", \" \").replace(\"?\", \" \") \\\n        .replace(\"  \",\" \").replace(\"  \",\" \") \\\n        .replace(')','').replace('(','') \\\n        .replace(\"—\",' ').replace(\"\\n\",' '). replace(\"\\t\",' ') \\\n        .replace('   ', ' ').replace('  ', ' ')\n            \n    return row","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:12.065849Z","iopub.execute_input":"2022-12-15T07:14:12.066578Z","iopub.status.idle":"2022-12-15T07:14:12.078638Z","shell.execute_reply.started":"2022-12-15T07:14:12.066542Z","shell.execute_reply":"2022-12-15T07:14:12.077618Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def train_model_bert(bert, save_path, df_train, df_test, num_epochs=1, type_model = 'bert', check_fold = 1):\n        text_train = df_train['Comment'].to_list()\n        label_train = df_train['Rating'].to_list()\n        text_test = df_test['Comment'].to_list()\n        label_test = df_test['Rating'].to_list()\n        \n        Data_train, attention_train = bert.tokenize(text_train, label_train)\n        Data_test, attention_test = bert.tokenize(text_test, label_test)\n        \n        dataloader_dict = {\n            'train' : Data_train,\n            'test' : Data_test\n        }\n        \n        check = 0\n        \n        for epoch in range(num_epochs + 1):\n            if epoch == 0:\n                print()\n                continue\n            print(\"Epoch {}/{} \".format(epoch, num_epochs))\n\n            for phase in ['train', 'test']:\n                if phase == 'train':\n                    bert.train()\n                    attention = attention_train\n                else:\n                    bert.eval()\n                    attention = attention_test\n\n                epoch_loss = 0.0\n                epoch_corrects = 0\n\n                for (inputs, labels), i_attention in tqdm(zip(dataloader_dict[phase], attention)):\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n                    i_attention = i_attention.to(device)\n\n                    bert.optimizer.zero_grad()\n\n                    with torch.set_grad_enabled(phase == 'train'):\n                        outputs = bert(inputs,i_attention)\n\n                        loss = bert.criterior(outputs, labels)\n\n                        _, preds = torch.max(outputs, 1)\n\n                        if phase == 'train':\n                            loss.backward()\n                            bert.optimizer.step()\n\n                        epoch_loss += loss.item() * inputs.size(0)\n                        epoch_corrects += torch.sum(preds == labels.data)\n                        del outputs\n                        torch.cuda.empty_cache()\n\n                epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)\n                epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)\n                \n                print(\"{} Loss: {:.4f}  Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n                \n                if check < epoch_acc and phase == 'test':\n                    check = epoch_acc\n                    torch.save(bert.state_dict(), \"./full_{}_model_fold{}.pth\".format(type_model, check_fold))","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:12.080547Z","iopub.execute_input":"2022-12-15T07:14:12.081029Z","iopub.status.idle":"2022-12-15T07:14:12.094386Z","shell.execute_reply.started":"2022-12-15T07:14:12.080977Z","shell.execute_reply":"2022-12-15T07:14:12.093403Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### train","metadata":{}},{"cell_type":"code","source":"df_train = shuffle(df_train)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:14:12.095934Z","iopub.execute_input":"2022-12-15T07:14:12.096269Z","iopub.status.idle":"2022-12-15T07:14:12.110468Z","shell.execute_reply.started":"2022-12-15T07:14:12.096236Z","shell.execute_reply":"2022-12-15T07:14:12.109592Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"b = BERT_classification()\ntrain_model_bert(b, './', df_train, df_val, num_epochs = 6, type_model = 'bert')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-15T07:14:12.111924Z","iopub.execute_input":"2022-12-15T07:14:12.113057Z","iopub.status.idle":"2022-12-15T07:56:15.647661Z","shell.execute_reply.started":"2022-12-15T07:14:12.113018Z","shell.execute_reply":"2022-12-15T07:56:15.646675Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50ff73f0f9b04eb99bd01da1d61c2cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/518M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4475b8e6ecfd4d78be38118e6a7b2f0f"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/874k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"181885190523494bbd7a7a22cd541ae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b710b25d34746a4bfb343c810237321"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/6 \n","output_type":"stream"},{"name":"stderr","text":"359it [06:07,  1.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.3061  Acc: 0.8834\n","output_type":"stream"},{"name":"stderr","text":"160it [00:39,  4.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"test Loss: 0.2560  Acc: 0.9097\nEpoch 2/6 \n","output_type":"stream"},{"name":"stderr","text":"359it [06:07,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.2422  Acc: 0.9173\n","output_type":"stream"},{"name":"stderr","text":"160it [00:40,  4.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"test Loss: 0.2343  Acc: 0.9195\nEpoch 3/6 \n","output_type":"stream"},{"name":"stderr","text":"359it [06:07,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.2084  Acc: 0.9306\n","output_type":"stream"},{"name":"stderr","text":"160it [00:39,  4.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"test Loss: 0.2333  Acc: 0.9244\nEpoch 4/6 \n","output_type":"stream"},{"name":"stderr","text":"359it [06:07,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.1724  Acc: 0.9436\n","output_type":"stream"},{"name":"stderr","text":"160it [00:39,  4.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"test Loss: 0.2232  Acc: 0.9322\nEpoch 5/6 \n","output_type":"stream"},{"name":"stderr","text":"359it [06:07,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.1517  Acc: 0.9514\n","output_type":"stream"},{"name":"stderr","text":"160it [00:39,  4.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"test Loss: 0.2483  Acc: 0.9306\nEpoch 6/6 \n","output_type":"stream"},{"name":"stderr","text":"359it [06:07,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"train Loss: 0.1266  Acc: 0.9611\n","output_type":"stream"},{"name":"stderr","text":"160it [00:40,  3.99it/s]","output_type":"stream"},{"name":"stdout","text":"test Loss: 0.2634  Acc: 0.9275\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict_bert(model, df_val):\n    text_sub = df_val['Comment'].to_list()\n    tmp = np.zeros(len(text_sub))\n    net = model\n    net.eval()\n    Data_sub, attention_sub = net.tokenize(text_sub, tmp)\n    attention = attention_sub\n\n    list_tmp = []\n    list_truth = np.array([])\n\n    for (inputs, labels), i_attention in tqdm(zip(Data_sub, attention)):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        i_attention = i_attention.to(device)\n        with torch.no_grad():\n\n            outputs = net(inputs,i_attention)\n\n            list_tmp.append(outputs)\n\n            _ , preds = torch.max(outputs, 1)\n\n            del outputs\n            torch.cuda.empty_cache()\n            gc.collect()\n\n            list_truth = np.append(list_truth,preds.cpu().numpy())\n    \n    list_prob = []\n    for i in range(len(list_tmp)):\n        probs = F.softmax(list_tmp[i], dim=1) # assuming logits has the shape [batch_size, nb_classes]\n        list_prob.append(probs)\n    \n    list_ans = []\n    for i in list_prob:\n        list_ans = np.append(list_ans,i.cpu().numpy()[:,1])\n        \n    return list_truth, list_ans","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:56:15.653079Z","iopub.execute_input":"2022-12-15T07:56:15.655043Z","iopub.status.idle":"2022-12-15T07:56:15.665017Z","shell.execute_reply.started":"2022-12-15T07:56:15.655013Z","shell.execute_reply":"2022-12-15T07:56:15.664124Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"b = BERT_classification()\nw = torch.load('/kaggle/working/full_bert_model_fold{}.pth'.format(1))\nb.load_state_dict(w)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:56:15.667715Z","iopub.execute_input":"2022-12-15T07:56:15.668357Z","iopub.status.idle":"2022-12-15T07:56:28.897432Z","shell.execute_reply.started":"2022-12-15T07:56:15.668296Z","shell.execute_reply":"2022-12-15T07:56:28.896436Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn.functional as F\n# text_test = df_val['Comment'].to_list()\nlist_predict_bert, list_ans = predict_bert(b, df_val)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-15T07:56:28.898746Z","iopub.execute_input":"2022-12-15T07:56:28.899861Z","iopub.status.idle":"2022-12-15T07:57:47.203616Z","shell.execute_reply.started":"2022-12-15T07:56:28.899822Z","shell.execute_reply":"2022-12-15T07:57:47.202516Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"160it [01:15,  2.13it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### submit","metadata":{}},{"cell_type":"code","source":"df_sub = pd.read_csv(TEST_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:57:47.228212Z","iopub.execute_input":"2022-12-15T07:57:47.228715Z","iopub.status.idle":"2022-12-15T07:57:47.325383Z","shell.execute_reply.started":"2022-12-15T07:57:47.228674Z","shell.execute_reply":"2022-12-15T07:57:47.324277Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nb = BERT_classification()\nw = torch.load('/kaggle/working/full_bert_model_fold{}.pth'.format(1))\nb.load_state_dict(w)\nlist_predict, list_ans1 = predict_bert(b, df_sub)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-12-15T07:57:47.326952Z","iopub.execute_input":"2022-12-15T07:57:47.327297Z","iopub.status.idle":"2022-12-15T07:59:17.966020Z","shell.execute_reply.started":"2022-12-15T07:57:47.327262Z","shell.execute_reply":"2022-12-15T07:59:17.964931Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n160it [01:14,  2.13it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"list_ans1","metadata":{"execution":{"iopub.status.busy":"2022-12-15T07:59:17.967843Z","iopub.execute_input":"2022-12-15T07:59:17.968227Z","iopub.status.idle":"2022-12-15T07:59:17.976066Z","shell.execute_reply.started":"2022-12-15T07:59:17.968189Z","shell.execute_reply":"2022-12-15T07:59:17.974960Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"array([0.29014805, 0.05337064, 0.97625148, ..., 0.0036939 , 0.91400576,\n       0.98500466])"},"metadata":{}}]},{"cell_type":"code","source":"df_sub['Rating'] = list_ans1","metadata":{"execution":{"iopub.status.busy":"2022-12-15T08:00:04.128366Z","iopub.execute_input":"2022-12-15T08:00:04.128803Z","iopub.status.idle":"2022-12-15T08:00:04.135205Z","shell.execute_reply.started":"2022-12-15T08:00:04.128766Z","shell.execute_reply":"2022-12-15T08:00:04.134167Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"df_sub[['RevId','Rating']].to_csv('submission_phobert.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T08:00:13.361469Z","iopub.execute_input":"2022-12-15T08:00:13.362153Z","iopub.status.idle":"2022-12-15T08:00:13.383235Z","shell.execute_reply.started":"2022-12-15T08:00:13.362114Z","shell.execute_reply":"2022-12-15T08:00:13.382236Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}